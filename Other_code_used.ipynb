{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28edaf30-8b19-41dd-a5f7-f0ed3a1932e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used as inspiration for generator functions\n",
    "# https://github.com/Azure/lstms_for_predictive_maintenance/blob/master/Deep%20Learning%20Basics%20for%20Predictive%20Maintenance.ipynb\n",
    "\n",
    "\n",
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    data_array = id_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_array[start:stop, :]\n",
    "        \n",
    "\n",
    "# generator for the sequences\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['id'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8feeb8-b15f-4159-8ccc-2bc5ebd5ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used as guide to build model, although they did classification\n",
    "\n",
    "# https://github.com/Azure/lstms_for_predictive_maintenance/blob/master/Deep%20Learning%20Basics%20for%20Predictive%20Maintenance.ipynb\n",
    "\n",
    "# build the network\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=nb_out, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d449e3-d8cf-444f-b8c8-ba7aacb64d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where I got the idea for what was kinda the best performing model\n",
    "# https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a81df23-6e52-4bd9-9ba4-71a54cd5d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to implement conditional scaling, also used some of the plotting code beacuse it looked a lot prettier than what I made\n",
    "# also borrowed the code for making RMSE functions\n",
    "\n",
    "# https://towardsdatascience.com/lstm-for-predictive-maintenance-of-turbofan-engines-f8c7791353f3\n",
    "\n",
    "def evaluate(y_true, y_hat, label='test'):\n",
    "    mse = mean_squared_error(y_true, y_hat)\n",
    "    rmse = np.sqrt(mse)\n",
    "    variance = r2_score(y_true, y_hat)\n",
    "    print('{} set RMSE:{}, R2:{}'.format(label, rmse, variance))\n",
    "    \n",
    "\n",
    "def plot_signal(df, signal_name, unit_nr=None):\n",
    "    plt.figure(figsize=(13,5))\n",
    "    \n",
    "    if unit_nr:\n",
    "        plt.plot('RUL', signal_name, \n",
    "                data=df[df['unit_nr']==unit_nr])\n",
    "    else:\n",
    "        for i in train['unit_nr'].unique():\n",
    "            if (i % 10 == 0):  # only ploting every 10th unit_nr\n",
    "                plt.plot('RUL', signal_name, \n",
    "                         data=df[df['unit_nr']==i])\n",
    "    plt.xlim(250, 0)  # reverse the x-axis so RUL counts down to zero\n",
    "    plt.xticks(np.arange(0, 275, 25))\n",
    "    plt.ylabel(signal_name)\n",
    "    plt.xlabel('Remaining Use fulLife')\n",
    "    plt.show()\n",
    "    \n",
    "    for sensor in sensor_names:\n",
    "    plot_signal(X_train_condition_scaled, sensor)\n",
    "    \n",
    "def plot_loss(fit_history):\n",
    "    plt.figure(figsize=(13,5))\n",
    "    plt.plot(range(1, len(fit_history.history['loss'])+1), fit_history.history['loss'], label='train')\n",
    "    plt.plot(range(1, len(fit_history.history['val_loss'])+1), fit_history.history['val_loss'], label='validate')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def condition_scaler(df_train, df_test, sensor_names):\n",
    "    # apply operating condition specific scaling\n",
    "    scaler = StandardScaler()\n",
    "    for condition in df_train['op_cond'].unique():\n",
    "        scaler.fit(df_train.loc[df_train['op_cond']==condition, sensor_names])\n",
    "        df_train.loc[df_train['op_cond']==condition, sensor_names] = scaler.transform(df_train.loc[df_train['op_cond']==condition, sensor_names])\n",
    "        df_test.loc[df_test['op_cond']==condition, sensor_names] = scaler.transform(df_test.loc[df_test['op_cond']==condition, sensor_names])\n",
    "    return df_train, df_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
